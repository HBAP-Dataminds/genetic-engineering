{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DrivenData - Genetic Engineering Attribution Challenge\n",
    "#### 12-07-2020, Jim set up Benchmark\n",
    "#### 12-10-2020, Vanessa added dataset links \n",
    "#### 12-19-2020, Group Hackthon collaborated efforts with RFC, Native Bayes,\n",
    "+ RBF and Neuro Network In progress (session 420 and 430)\n",
    "\n",
    "#### future work\n",
    "+ impute N in the DNA sequence\n",
    "    - Idea 1 during the n-gram feature generation phase, add the N related feature occurance\n",
    "+ implement the ability to show progress of space search\n",
    "+ implement the ability to generate and eliminate features\n",
    "+ implement test out RBF and Neuron Network estimator.\n",
    "    - use guassian process for RBF?\n",
    "\n",
    "#### additional functionality extension\n",
    "+ figure out how to premutate more than 5 n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "print(\"Libraries Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Following code for reading off of JMO Harddrive\n",
    "\n",
    "#DATA_DIR = 'C:/Users/jorrison/Documents/Home/Harvard/DataMinds/Genetic Engineering/Data/'\n",
    "\n",
    "#train_values = pd.read_csv(DATA_DIR + 'train_values.csv', index_col='sequence_id')\n",
    "#train_labels = pd.read_csv(DATA_DIR + 'train_labels.csv', index_col='sequence_id')\n",
    "#test_values  = pd.read_csv(DATA_DIR + 'test_values.csv' , index_col='sequence_id')\n",
    "\n",
    "#train_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read local by roland\n",
    "\n",
    "train_labels = pd.read_csv('train_labels.csv', index_col='sequence_id')\n",
    "train_values = pd.read_csv('train_values.csv', index_col='sequence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remote read\n",
    "\n",
    "#train_values = pd.read_csv('https://www.dropbox.com/s/3atnfvr65oh0y4y/train_values.csv?dl=1', index_col='sequence_id')\n",
    "#train_labels = pd.read_csv('https://www.dropbox.com/s/71yg5f3dbc4h2yd/train_labels.csv?dl=1', index_col='sequence_id')\n",
    "#test_values  = pd.read_csv('https://www.dropbox.com/s/4g1astwaccer5lz/test_values.csv?dl=1' , index_col='sequence_id')\n",
    "\n",
    "# train_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 110 Explore the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 120 remove the lab has the highest frequency occurance in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "largelab_rownum = train_labels[ train_labels['I7FXTVDP'] > 0 ].index\n",
    "largelab_rownum.shape\n",
    "largelab_rownum\n",
    "train_labels_dropped = train_labels.drop(largelab_rownum)\n",
    "train_values_dropped = train_values.drop(largelab_rownum)\n",
    "train_labels_dropped.shape\n",
    "train_values_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 130 reduce the output labels dimension to string label text.\n",
    "#lab_ids = pd.DataFrame(train_labels.idxmax(axis=1), columns = ['lab_id'])\n",
    "lab_ids = pd.DataFrame(train_labels_dropped.idxmax(axis=1), columns = ['lab_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = train_values.sequence.apply(len)\n",
    "\n",
    "sequence_lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 140 visualize number of sequence code with length of 2500 or less.\n",
    "sequence_lengths.plot(kind='hist',\n",
    "                     title= 'Distribution of DNA Sequence Length', \n",
    "                     bins = 2500,\n",
    "                     xlim = (0, 2500))   # Suppress a bar longer than 20,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 150 visualize number of genetic sequence code with length of 1700 or less\n",
    "sequence_lengths[sequence_lengths<1700].plot(kind='hist',\n",
    "                     title= 'Distribution of DNA Sequence Length', \n",
    "                     bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 160 extrapolate data with genetic sequence length 1700 or less\n",
    "\n",
    "index_ferdous = sequence_lengths[sequence_lengths<1700].index\n",
    "df_ferdous_values = train_values_dropped.loc[index_ferdous]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 170 visualize the value occurance of other 39 features contained in training values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_binary_features = train_values.iloc[:, 1:].mean().sort_values()\n",
    "\n",
    "ax = sorted_binary_features.plot(kind = 'barh',\n",
    "                                stacked = True,\n",
    "                                figsize = (5,12),\n",
    "                                title   = 'Prevalence of Binary Features')\n",
    "ax.set_xlabel('Proportion of sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 175 inspect the output lab occurrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.sum().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 180 Explore the Test Data\n",
    "\n",
    "skipped, since we can't check the test data result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 190 Explore Training Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the column with the max value in each row\n",
    "\n",
    "lab_ids = pd.DataFrame(train_labels.idxmax(axis=1), columns = ['lab_id'])\n",
    "lab_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the distribution of lab prevalence in the training set\n",
    "lab_ids['lab_id'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort lab ids by prevalence\n",
    "(lab_ids['lab_id'].value_counts(normalize = True).sort_values(ascending=False)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lab_ids_ferdous = lab_ids.loc[index_ferdous]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 200 Enrich Data Sets\n",
    "#### 210 Construct Features from DNA Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bases = set(''.join(train_values.sequence.values))\n",
    "bases ={'A', 'C', 'G', 'N', 'T'}\n",
    "bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "n = 5\n",
    "subsequences = [''.join(permutation) for permutation in permutations(bases, r=n)]\n",
    "\n",
    "len(subsequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 220 Reduced Permutation to test out the effect. seems the 2-gram approach is not as good as 4 grams\n",
    "n = 2\n",
    "subsequences = [''.join(permutation) for permutation in permutations(bases, r=n)]\n",
    "print(f\"Number of subsequences: {len(subsequences)}\")\n",
    "subsequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 230 Example of built-in count method on strings\n",
    "Because it's non-overlapping, \"atta\" is only counted twice\n",
    "\"gattattattaca\".count(\"atta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 240 Counting the Subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_features(data, subsequences):\n",
    "    \"\"\"Generates counts for each subsequence.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame): The data you want to create features from. Must subsequences (list): A list of subsequences to count\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with one column for each subsequence\n",
    "    \"\"\"\n",
    "    \n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    for subseq in subsequences:\n",
    "        features[subseq] = data.sequence.str.count(subseq)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate n-gram features on our training set (ferdous training set)\n",
    "ngram_features = get_ngram_features(train_values, subsequences)\n",
    "ngram_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_features.value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate n-gram features on our training set\n",
    "ngram_features = get_ngram_features(train_values, subsequences)\n",
    "ngram_features.head()\n",
    "ngram_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ngram_features.join(train_values.drop('sequence', axis=1))\n",
    "\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "default_scaler = StandardScaler()\n",
    "default_scaler.fit(all_features)\n",
    "all_features_scaled = default_scaler.transform(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 300 Processing Top Ten Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10_accuracy_scorer(estimator, X, y):\n",
    "    \"\"\"A custom scorer that evaluates a model on whether the correct label the top 10 most probably predictions.\n",
    "    \n",
    "    Args:\n",
    "        estimatory (sklearn estimator): The skLearn model that should be evaluatioed \n",
    "        X (numpy array): The validation data.\n",
    "        y (numpy array): The ground truth lables.\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy of the model as defined by the proportion of prediction in which the correct label was in Top 10\n",
    "    \"\"\"\n",
    "    # predict the probabilities across all possible labels for rows in our training set\n",
    "    probas = estimator.predict_proba(X)\n",
    "    \n",
    "    # get the indices for top 10 predictions for each row; these are these are the last ten in each row\n",
    "    # Note: We use argpartition, which is O(n), vs argsort, which uses the quicksort algorithm \n",
    "    # by default and is O(n^2) in the worst case. We can do this because we only need the top ten\n",
    "    # partitioned, not in sorted order.\n",
    "    \n",
    "    # Documentation: https://numpy.org/doc/1.18/reference/generated/numpy.argpartition.html\n",
    "    top10_idx = np.argpartition(probas, -10, axis=1)[:, -10:]\n",
    "    \n",
    "    # index into the classes list using the top ten indices to get the class names\n",
    "    top10_preds = estimator.classes_[top10_idx]\n",
    "\n",
    "    # check if y-true is in top 10 for each set of predictions\n",
    "    mask = top10_preds == y.reshape((y.size, 1))\n",
    "    \n",
    "    # take the mean\n",
    "    top_10_accuracy = mask.any(axis=1).mean()\n",
    " \n",
    "    return top_10_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 350 Break apart Training Data Set into Train-Validate-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_test_split will generally split the data set as per describe the test_size\n",
    "# However under the condition when there is only one set of data available, train_test_split will return an error.\n",
    "# for quickSplit, it will solve the issue as per general train_test_split except for the single record,\n",
    "# which quickSplit will return the exact value back as train and test.\n",
    "\n",
    "def quickSplit(X,y, TEST_SIZE=0.2):\n",
    "    if X.shape[0] ==1:\n",
    "        X_train = X\n",
    "        X_test  = X\n",
    "        y_train = y\n",
    "        y_test  = y\n",
    "    else:\n",
    "        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = TEST_SIZE)\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSplit(df_X,df_y,col_label = \"lab_id\",TEST_SIZE=0.2):\n",
    "    ls_y_labels = list(df_y[col_label].unique())\n",
    "\n",
    "    X_train = pd.DataFrame()\n",
    "    X_test  = pd.DataFrame()\n",
    "    y_train = pd.DataFrame()\n",
    "    y_test  = pd.DataFrame()\n",
    "    \n",
    "    for label in ls_y_labels:\n",
    "        y = df_y[df_y[col_label] == label]\n",
    "        X = df_X.loc[y.index]\n",
    "        \n",
    "        Xa,Xb,ya,yb  =  quickSplit(X,y,TEST_SIZE)\n",
    "\n",
    "        X_train = X_train.append(Xa)\n",
    "        X_test  = X_test.append(Xb)\n",
    "        y_train = y_train.append(ya)\n",
    "        y_test  = y_test.append(yb)\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "X_train,X_validate,y_train,y_validate = randomSplit(all_features,lab_ids,TEST_SIZE=0.2)\n",
    "end   = time.perf_counter()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 351 a quick validation to check both the train and validate have covered the whole spectrum of the output lab labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No of labs in training set: '+str(y_train[\"lab_id\"].shape[0]))\n",
    "print('No of labs in training set: '+str(y_validate[\"lab_id\"].shape[0]))\n",
    "\n",
    "print('No of labs in training set: '+str(len(list(y_train[\"lab_id\"].unique()))))\n",
    "print('No of labs in training set: '+str(len(list(y_validate[\"lab_id\"].unique()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 352 the old way of splitting the data without validating the proportion.\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(all_features, lab_ids, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename our feature array\n",
    "X = X_train                        # 80% of the 63,000 original Samples for 40,324\n",
    "\n",
    "# Create our labels\n",
    "y = y_train.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400 ML Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 410 Random Forest (functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators =400,\n",
    "                            class_weight=\"balanced\",\n",
    "                            max_depth =10,\n",
    "                            bootstrap =False,\n",
    "                            n_jobs = 5)\n",
    "# fit our model\n",
    "start = time.perf_counter()\n",
    "rf.fit(X,y)\n",
    "end   = time.perf_counter()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "result= rf.score(X_validate, y_validate)\n",
    "end   = time.perf_counter()\n",
    "\n",
    "print(\"Single value accuracy rating:\" + str(result))\n",
    "print(\"Single value process time:\"+ str(end-start))\n",
    "\n",
    "start = time.perf_counter()\n",
    "top10_result=top10_accuracy_scorer(rf, X_validate, y_validate.to_numpy())\n",
    "end   = time.perf_counter()\n",
    "\n",
    "print(\"Top 10 value accuracy:\"+str(top10_result))\n",
    "print(\"Top 10 value process time:\"+str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ changing the estimator number to 400 improved the result significantly\n",
    "+ increase the depth to 10 improved the result.\n",
    "+ scaling the features improve the result by 4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 420 Keras neuro-network (not functional yet, use with caution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_features_scaled).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "mohammed = Sequential()\n",
    "mohammed.add(Dense(160,activation = \"relu\",input_dim =159))\n",
    "mohammed.add(Dense(160,activation = \"relu\",input_dim =159))\n",
    "mohammed.add(Dense(160,activation = \"relu\",input_dim =159))\n",
    "mohammed.add(Dense(160,activation = \"relu\",input_dim =159))\n",
    "mohammed.add(Dense(160,activation = \"relu\",input_dim =159))\n",
    "mohammed.add(Dense(160,activation = \"relu\",input_dim =159))\n",
    "mohammed.add(Dense(160,activation = \"relu\",input_dim =159))\n",
    "mohammed.add(Dense(160,activation = \"relu\",input_dim =159))\n",
    "mohammed.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "mohammed.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 430 X boost (functional but took a long time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=400,learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "result= xgb.score(X_holdout, y_holdout)\n",
    "end   = time.perf_counter()\n",
    "\n",
    "print(result)\n",
    "print(end-start)\n",
    "\n",
    "start = time.perf_counter()\n",
    "top10_result=top10_accuracy_scorer(xgb, X_holdout, y_holdout.to_numpy())\n",
    "end   = time.perf_counter()\n",
    "\n",
    "print(top10_result)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 440 MultinomialNB (functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "param_metric = {\"alpha\":[0.3,0.1,0.03,0.005],\n",
    "               \"fit_prior\":[True,False]}\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "GS_MNB = GridSearchCV(MNB, \n",
    "                      param_grid=param_metric,\n",
    "                      scoring=\"accuracy\",\n",
    "                      n_jobs= 4,\n",
    "                      verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal = CalibratedClassifierCV(GS_MNB, cv=5, method=\"isotonic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_MNB.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "result= GS_MNB.score(X_validate, y_validate)\n",
    "end   = time.perf_counter()\n",
    "\n",
    "print(\"Single value accuracy rating:\" + str(result))\n",
    "print(\"Single value process time:\"+ str(end-start))\n",
    "\n",
    "start = time.perf_counter()\n",
    "top10_result=top10_accuracy_scorer(GS_MNB, X_validate, y_validate.to_numpy())\n",
    "end   = time.perf_counter()\n",
    "\n",
    "print(\"Top 10 value accuracy:\"+str(top10_result))\n",
    "print(\"Top 10 value process time:\"+str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 450 RBF SVM (not functional yet, use with caution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "param_metric={'kernal':('linear','rbf'),\n",
    "             'C':[1,10]}\n",
    "\n",
    "svc = SVC(C = 1,kernel = 'rbf',gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = ('linear','rbf')\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(kernal = k_range,\n",
    "                  gamma=gamma_range,\n",
    "                  C=C_range)\n",
    "param_metric = {'gamma':[0.01,0.1,1,10],\n",
    "                'C':[0.01,0.1,1,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsRBF = GridSearchCV(svc,\n",
    "                     param_grid=param_metric,\n",
    "                     scoring=\"accuracy\",\n",
    "                     n_jobs= 4,\n",
    "                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsRBF.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 500 Make Predictions (untouched, only useful if we are submitting to DataDriven site.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values_filled = test_values.fillna(0)\n",
    "#test_values_filled = test_values.dropna(0)\n",
    "test_values_filled.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ngram_features = get_ngram_features(test_values, subsequences)\n",
    "test_ngram_features = get_ngram_features(test_values_filled, subsequences)\n",
    "all_test_features   = test_ngram_features.join(test_values_filled.drop('sequence', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Visualize the building blocks to \"all_test_features\" and \"all_test_features\"\n",
    "\n",
    "#test_values.describe()\n",
    "\n",
    "#test_ngram_features.describe()\n",
    "\n",
    "all_test_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = rf.predict_proba(all_test_features)\n",
    "\n",
    "# Examine first row\n",
    "probas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 520 Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv(DATA_DIR + 'submission_format_3TFRxH6.csv', index_col='sequence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values_filled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert submission_format.shape == probas.shape\n",
    "#assert (rf.classes_ == submission_format.columns).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
